{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/software/'\n",
    "import sys\n",
    "import gc\n",
    "# assuming data, models, engine in flicc directory:\n",
    "flicc_path = os.path.realpath(\"__file__\").split('grid_search')[0]\n",
    "sys.path.append(flicc_path)\n",
    "import torch\n",
    "from data import ClimateDataset\n",
    "from models import ClassificationModel\n",
    "from engine import Engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint='microsoft/deberta-v2-xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'test_acc':[],\n",
    "           'test_f1':[],\n",
    "           'eval_acc':[],\n",
    "           'eval_f1':[],\n",
    "           'lr':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search microsoft/deberta-v2-xlarge, learning rate 1e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa2da251e8e4291a065bbbbd27e2c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6b8eacf97742fc9a761721dc32b8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1617e072494b66bf0dc3d125807214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6afdd692ff4de2a8fa6a6910bae00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00cd10b0a328488280692e9c123117fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b27185309b4c4cb0ad2ff921e2674a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/338 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.5533\tVal Loss:\t2.2040\tAccuracy:\t0.1420\tF1:\t0.0814\tTest Accuracy:\t0.1587\tTest F1:\t0.0957 *\n",
      "2 / 30: Train Loss:\t0.5536\tVal Loss:\t2.2011\tAccuracy:\t0.1450\tF1:\t0.0847\tTest Accuracy:\t0.1587\tTest F1:\t0.0968 *\n",
      "3 / 30: Train Loss:\t0.5496\tVal Loss:\t2.1976\tAccuracy:\t0.1598\tF1:\t0.0893\tTest Accuracy:\t0.1746\tTest F1:\t0.0968 *\n",
      "4 / 30: Train Loss:\t0.5501\tVal Loss:\t2.1942\tAccuracy:\t0.1598\tF1:\t0.0871\tTest Accuracy:\t0.1799\tTest F1:\t0.0932\n",
      "5 / 30: Train Loss:\t0.5494\tVal Loss:\t2.1901\tAccuracy:\t0.1657\tF1:\t0.0852\tTest Accuracy:\t0.1905\tTest F1:\t0.0932\n",
      "6 / 30: Train Loss:\t0.5478\tVal Loss:\t2.1830\tAccuracy:\t0.1746\tF1:\t0.0870\tTest Accuracy:\t0.2116\tTest F1:\t0.1039\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 1e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d37fee9479436c9240ae2804cc8702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.5492\tVal Loss:\t2.1691\tAccuracy:\t0.1834\tF1:\t0.0789\tTest Accuracy:\t0.2063\tTest F1:\t0.0914 *\n",
      "2 / 30: Train Loss:\t0.5414\tVal Loss:\t2.1374\tAccuracy:\t0.1893\tF1:\t0.0723\tTest Accuracy:\t0.1958\tTest F1:\t0.0695\n",
      "3 / 30: Train Loss:\t0.5318\tVal Loss:\t2.1146\tAccuracy:\t0.1982\tF1:\t0.0762\tTest Accuracy:\t0.2063\tTest F1:\t0.0722\n",
      "4 / 30: Train Loss:\t0.5277\tVal Loss:\t2.0950\tAccuracy:\t0.1923\tF1:\t0.0660\tTest Accuracy:\t0.2063\tTest F1:\t0.0705\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843209b5aff74abf9079cab89c836643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/338 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.5350\tVal Loss:\t2.0734\tAccuracy:\t0.1953\tF1:\t0.0642\tTest Accuracy:\t0.2116\tTest F1:\t0.0716 *\n",
      "2 / 30: Train Loss:\t0.5159\tVal Loss:\t2.0431\tAccuracy:\t0.1923\tF1:\t0.0574\tTest Accuracy:\t0.2222\tTest F1:\t0.0759\n",
      "3 / 30: Train Loss:\t0.5063\tVal Loss:\t2.0256\tAccuracy:\t0.2071\tF1:\t0.0685\tTest Accuracy:\t0.2222\tTest F1:\t0.0746 *\n",
      "4 / 30: Train Loss:\t0.5028\tVal Loss:\t2.0013\tAccuracy:\t0.2337\tF1:\t0.0869\tTest Accuracy:\t0.2381\tTest F1:\t0.0869 *\n",
      "5 / 30: Train Loss:\t0.4959\tVal Loss:\t1.9485\tAccuracy:\t0.2870\tF1:\t0.1192\tTest Accuracy:\t0.2857\tTest F1:\t0.1133 *\n",
      "6 / 30: Train Loss:\t0.4769\tVal Loss:\t1.8431\tAccuracy:\t0.3343\tF1:\t0.1679\tTest Accuracy:\t0.3439\tTest F1:\t0.1658 *\n",
      "7 / 30: Train Loss:\t0.4411\tVal Loss:\t1.6413\tAccuracy:\t0.4556\tF1:\t0.2728\tTest Accuracy:\t0.4180\tTest F1:\t0.2281 *\n",
      "8 / 30: Train Loss:\t0.3923\tVal Loss:\t1.4392\tAccuracy:\t0.5000\tF1:\t0.3068\tTest Accuracy:\t0.4868\tTest F1:\t0.2893 *\n",
      "9 / 30: Train Loss:\t0.3461\tVal Loss:\t1.2855\tAccuracy:\t0.5680\tF1:\t0.3833\tTest Accuracy:\t0.5714\tTest F1:\t0.3822 *\n",
      "10 / 30: Train Loss:\t0.3098\tVal Loss:\t1.1691\tAccuracy:\t0.6272\tF1:\t0.4396\tTest Accuracy:\t0.6508\tTest F1:\t0.4585 *\n",
      "11 / 30: Train Loss:\t0.2782\tVal Loss:\t1.0622\tAccuracy:\t0.6420\tF1:\t0.4534\tTest Accuracy:\t0.6720\tTest F1:\t0.4736 *\n",
      "12 / 30: Train Loss:\t0.2504\tVal Loss:\t0.9713\tAccuracy:\t0.6716\tF1:\t0.4766\tTest Accuracy:\t0.6825\tTest F1:\t0.4867 *\n",
      "13 / 30: Train Loss:\t0.2267\tVal Loss:\t0.8996\tAccuracy:\t0.6775\tF1:\t0.5231\tTest Accuracy:\t0.6931\tTest F1:\t0.4963 *\n",
      "14 / 30: Train Loss:\t0.2084\tVal Loss:\t0.8437\tAccuracy:\t0.7130\tF1:\t0.5633\tTest Accuracy:\t0.7037\tTest F1:\t0.5298 *\n",
      "15 / 30: Train Loss:\t0.1957\tVal Loss:\t0.7974\tAccuracy:\t0.7278\tF1:\t0.5809\tTest Accuracy:\t0.7037\tTest F1:\t0.5298 *\n",
      "16 / 30: Train Loss:\t0.1818\tVal Loss:\t0.7643\tAccuracy:\t0.7278\tF1:\t0.5820\tTest Accuracy:\t0.6931\tTest F1:\t0.5231 *\n",
      "17 / 30: Train Loss:\t0.1719\tVal Loss:\t0.7825\tAccuracy:\t0.7337\tF1:\t0.6296\tTest Accuracy:\t0.6878\tTest F1:\t0.5175 *\n",
      "18 / 30: Train Loss:\t0.1738\tVal Loss:\t0.7306\tAccuracy:\t0.7426\tF1:\t0.6135\tTest Accuracy:\t0.7407\tTest F1:\t0.6001\n",
      "19 / 30: Train Loss:\t0.1591\tVal Loss:\t0.7169\tAccuracy:\t0.7574\tF1:\t0.6413\tTest Accuracy:\t0.7196\tTest F1:\t0.5671 *\n",
      "20 / 30: Train Loss:\t0.1508\tVal Loss:\t0.6976\tAccuracy:\t0.7544\tF1:\t0.6436\tTest Accuracy:\t0.7143\tTest F1:\t0.5629 *\n",
      "21 / 30: Train Loss:\t0.1441\tVal Loss:\t0.6830\tAccuracy:\t0.7751\tF1:\t0.6870\tTest Accuracy:\t0.7302\tTest F1:\t0.6043 *\n",
      "22 / 30: Train Loss:\t0.1408\tVal Loss:\t0.6721\tAccuracy:\t0.7604\tF1:\t0.6642\tTest Accuracy:\t0.7249\tTest F1:\t0.6022\n",
      "23 / 30: Train Loss:\t0.1316\tVal Loss:\t0.6841\tAccuracy:\t0.7722\tF1:\t0.6858\tTest Accuracy:\t0.7354\tTest F1:\t0.6090\n",
      "24 / 30: Train Loss:\t0.1270\tVal Loss:\t0.6661\tAccuracy:\t0.7692\tF1:\t0.6809\tTest Accuracy:\t0.7090\tTest F1:\t0.5928\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.5261\tVal Loss:\t2.0428\tAccuracy:\t0.1953\tF1:\t0.0610\tTest Accuracy:\t0.2169\tTest F1:\t0.0686 *\n",
      "2 / 30: Train Loss:\t0.5078\tVal Loss:\t2.0229\tAccuracy:\t0.2012\tF1:\t0.0692\tTest Accuracy:\t0.2116\tTest F1:\t0.0725 *\n",
      "3 / 30: Train Loss:\t0.4965\tVal Loss:\t1.9620\tAccuracy:\t0.2604\tF1:\t0.1126\tTest Accuracy:\t0.2540\tTest F1:\t0.1040 *\n",
      "4 / 30: Train Loss:\t0.4618\tVal Loss:\t1.6512\tAccuracy:\t0.4615\tF1:\t0.2828\tTest Accuracy:\t0.4286\tTest F1:\t0.2544 *\n",
      "5 / 30: Train Loss:\t0.3739\tVal Loss:\t1.3105\tAccuracy:\t0.5858\tF1:\t0.3981\tTest Accuracy:\t0.5608\tTest F1:\t0.3741 *\n",
      "6 / 30: Train Loss:\t0.2962\tVal Loss:\t1.1072\tAccuracy:\t0.6391\tF1:\t0.4493\tTest Accuracy:\t0.6138\tTest F1:\t0.4360 *\n",
      "7 / 30: Train Loss:\t0.2483\tVal Loss:\t0.9525\tAccuracy:\t0.6775\tF1:\t0.5320\tTest Accuracy:\t0.6984\tTest F1:\t0.5230 *\n",
      "8 / 30: Train Loss:\t0.2147\tVal Loss:\t0.8624\tAccuracy:\t0.7101\tF1:\t0.5721\tTest Accuracy:\t0.7090\tTest F1:\t0.5550 *\n",
      "9 / 30: Train Loss:\t0.1898\tVal Loss:\t0.8059\tAccuracy:\t0.7337\tF1:\t0.6229\tTest Accuracy:\t0.7196\tTest F1:\t0.5611 *\n",
      "10 / 30: Train Loss:\t0.1711\tVal Loss:\t0.7472\tAccuracy:\t0.7367\tF1:\t0.6222\tTest Accuracy:\t0.7196\tTest F1:\t0.5762\n",
      "11 / 30: Train Loss:\t0.1532\tVal Loss:\t0.7046\tAccuracy:\t0.7396\tF1:\t0.6392\tTest Accuracy:\t0.7249\tTest F1:\t0.6025 *\n",
      "12 / 30: Train Loss:\t0.1433\tVal Loss:\t0.6846\tAccuracy:\t0.7426\tF1:\t0.6589\tTest Accuracy:\t0.7407\tTest F1:\t0.6272 *\n",
      "13 / 30: Train Loss:\t0.1311\tVal Loss:\t0.6687\tAccuracy:\t0.7692\tF1:\t0.6894\tTest Accuracy:\t0.7460\tTest F1:\t0.6871 *\n",
      "14 / 30: Train Loss:\t0.1148\tVal Loss:\t0.6833\tAccuracy:\t0.7899\tF1:\t0.7093\tTest Accuracy:\t0.7460\tTest F1:\t0.6770 *\n",
      "15 / 30: Train Loss:\t0.1026\tVal Loss:\t0.6714\tAccuracy:\t0.7840\tF1:\t0.7009\tTest Accuracy:\t0.7196\tTest F1:\t0.6521\n",
      "16 / 30: Train Loss:\t0.0944\tVal Loss:\t0.6530\tAccuracy:\t0.7929\tF1:\t0.7262\tTest Accuracy:\t0.7460\tTest F1:\t0.6953 *\n",
      "17 / 30: Train Loss:\t0.0849\tVal Loss:\t0.6596\tAccuracy:\t0.7840\tF1:\t0.7258\tTest Accuracy:\t0.7460\tTest F1:\t0.6867\n",
      "18 / 30: Train Loss:\t0.0779\tVal Loss:\t0.6618\tAccuracy:\t0.7811\tF1:\t0.7362\tTest Accuracy:\t0.7354\tTest F1:\t0.6877 *\n",
      "19 / 30: Train Loss:\t0.0677\tVal Loss:\t0.6926\tAccuracy:\t0.7988\tF1:\t0.7552\tTest Accuracy:\t0.7249\tTest F1:\t0.6624 *\n",
      "20 / 30: Train Loss:\t0.0658\tVal Loss:\t0.6985\tAccuracy:\t0.7899\tF1:\t0.7563\tTest Accuracy:\t0.7354\tTest F1:\t0.6847 *\n",
      "21 / 30: Train Loss:\t0.0564\tVal Loss:\t0.7848\tAccuracy:\t0.7663\tF1:\t0.7553\tTest Accuracy:\t0.6984\tTest F1:\t0.6291\n",
      "22 / 30: Train Loss:\t0.0553\tVal Loss:\t0.7017\tAccuracy:\t0.7870\tF1:\t0.7671\tTest Accuracy:\t0.7354\tTest F1:\t0.7028 *\n",
      "23 / 30: Train Loss:\t0.0481\tVal Loss:\t0.7414\tAccuracy:\t0.7781\tF1:\t0.7636\tTest Accuracy:\t0.7354\tTest F1:\t0.6798\n",
      "24 / 30: Train Loss:\t0.0472\tVal Loss:\t0.6640\tAccuracy:\t0.8077\tF1:\t0.7714\tTest Accuracy:\t0.7513\tTest F1:\t0.6883 *\n",
      "25 / 30: Train Loss:\t0.0381\tVal Loss:\t0.8131\tAccuracy:\t0.7781\tF1:\t0.7564\tTest Accuracy:\t0.7196\tTest F1:\t0.6652\n",
      "26 / 30: Train Loss:\t0.0321\tVal Loss:\t0.7222\tAccuracy:\t0.7929\tF1:\t0.7709\tTest Accuracy:\t0.7460\tTest F1:\t0.6995\n",
      "27 / 30: Train Loss:\t0.0311\tVal Loss:\t0.6895\tAccuracy:\t0.8254\tF1:\t0.8001\tTest Accuracy:\t0.7619\tTest F1:\t0.7284 *\n",
      "28 / 30: Train Loss:\t0.0273\tVal Loss:\t0.7593\tAccuracy:\t0.7781\tF1:\t0.7586\tTest Accuracy:\t0.7090\tTest F1:\t0.6837\n",
      "29 / 30: Train Loss:\t0.0252\tVal Loss:\t0.8002\tAccuracy:\t0.7692\tF1:\t0.7501\tTest Accuracy:\t0.7302\tTest F1:\t0.6914\n",
      "30 / 30: Train Loss:\t0.0238\tVal Loss:\t0.7402\tAccuracy:\t0.7988\tF1:\t0.7770\tTest Accuracy:\t0.7513\tTest F1:\t0.7206\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.4927\tVal Loss:\t1.4621\tAccuracy:\t0.5207\tF1:\t0.3290\tTest Accuracy:\t0.5079\tTest F1:\t0.3243 *\n",
      "2 / 30: Train Loss:\t0.4317\tVal Loss:\t1.7654\tAccuracy:\t0.3018\tF1:\t0.1894\tTest Accuracy:\t0.3016\tTest F1:\t0.1794\n",
      "3 / 30: Train Loss:\t0.4442\tVal Loss:\t1.7204\tAccuracy:\t0.3077\tF1:\t0.1646\tTest Accuracy:\t0.3069\tTest F1:\t0.1663\n",
      "4 / 30: Train Loss:\t0.4842\tVal Loss:\t2.0843\tAccuracy:\t0.1982\tF1:\t0.0368\tTest Accuracy:\t0.1958\tTest F1:\t0.0364\n",
      "No improvement for 3 epochs. Stopping early.\n"
     ]
    }
   ],
   "source": [
    "r = 8\n",
    "a = 8\n",
    "lora_dropout = 0.0\n",
    "\n",
    "learning_rates = [1.0e-6, 1.0e-5, 5.0e-5 ,1.0e-4, 1.0e-3]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'Grid search {model_checkpoint}, learning rate {lr}')\n",
    "    data = ClimateDataset(model_to_train=3,model_checkpoint=model_checkpoint,dataset_url=flicc_path,batch_size=8)\n",
    "    data.setup_dataloaders()\n",
    "    model = ClassificationModel(model_checkpoint=data.model_checkpoint,\n",
    "                                num_labels=data.num_labels,\n",
    "                                quantized_model=True,\n",
    "                                lora=True,\n",
    "                                r=r,\n",
    "                                alpha=a,\n",
    "                                dropout=lora_dropout)\n",
    "    trainer = Engine(epochs=30,labels=data.labels)\n",
    "    trainer.model = model.model\n",
    "    trainer.run(lr=lr,\n",
    "                wd=0.0,\n",
    "                train_dataloader=data.train_dataloader,\n",
    "                eval_dataloader=data.eval_dataloader,\n",
    "                test_dataloader=data.test_dataloader,\n",
    "                accumulation_steps=4,\n",
    "                early_stop=3,\n",
    "                is_quantized=True)\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    del data, model, trainer\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
