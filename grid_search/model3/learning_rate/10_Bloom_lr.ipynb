{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/software/'\n",
    "import sys\n",
    "import gc\n",
    "# assuming data, models, engine in flicc directory:\n",
    "flicc_path = os.path.realpath(\"__file__\").split('grid_search')[0]\n",
    "sys.path.append(flicc_path)\n",
    "import torch\n",
    "from data import ClimateDataset\n",
    "from models import ClassificationModel\n",
    "from engine import Engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint='bigscience/bloom-560m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'test_acc':[],\n",
    "           'test_f1':[],\n",
    "           'eval_acc':[],\n",
    "           'eval_f1':[],\n",
    "           'lr':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search bigscience/bloom-560m, learning rate 1e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce857a7f5906440b9fe144b65d572596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acabd36b905b4bd1b85bf4c56e92a7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24228645ce654948b81bb1022bab06c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9667a5c455c54334aa05da1a79214960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ffb443131e40288bab7c521231b4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21a653647a844ca87306ea171c5807a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/338 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t13.7366\tVal Loss:\t55.1360\tAccuracy:\t0.0355\tF1:\t0.0076\tTest Accuracy:\t0.0370\tTest F1:\t0.0081 *\n",
      "2 / 30: Train Loss:\t13.4806\tVal Loss:\t54.1170\tAccuracy:\t0.0355\tF1:\t0.0076\tTest Accuracy:\t0.0370\tTest F1:\t0.0081\n",
      "3 / 30: Train Loss:\t13.2280\tVal Loss:\t53.1107\tAccuracy:\t0.0355\tF1:\t0.0076\tTest Accuracy:\t0.0370\tTest F1:\t0.0081\n",
      "4 / 30: Train Loss:\t12.9744\tVal Loss:\t52.0798\tAccuracy:\t0.0355\tF1:\t0.0077\tTest Accuracy:\t0.0370\tTest F1:\t0.0081 *\n",
      "5 / 30: Train Loss:\t12.7169\tVal Loss:\t51.0341\tAccuracy:\t0.0355\tF1:\t0.0077\tTest Accuracy:\t0.0370\tTest F1:\t0.0081\n",
      "6 / 30: Train Loss:\t12.4562\tVal Loss:\t49.9937\tAccuracy:\t0.0355\tF1:\t0.0077\tTest Accuracy:\t0.0370\tTest F1:\t0.0081\n",
      "7 / 30: Train Loss:\t12.1957\tVal Loss:\t48.9584\tAccuracy:\t0.0355\tF1:\t0.0077\tTest Accuracy:\t0.0370\tTest F1:\t0.0081\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 1e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5b88f33a8a4a779063a27ea1684f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/189 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t12.6312\tVal Loss:\t45.9013\tAccuracy:\t0.0444\tF1:\t0.0187\tTest Accuracy:\t0.0370\tTest F1:\t0.0082 *\n",
      "2 / 30: Train Loss:\t10.3437\tVal Loss:\t38.8798\tAccuracy:\t0.1538\tF1:\t0.0717\tTest Accuracy:\t0.1111\tTest F1:\t0.0501 *\n",
      "3 / 30: Train Loss:\t8.9455\tVal Loss:\t33.6878\tAccuracy:\t0.1538\tF1:\t0.0721\tTest Accuracy:\t0.1217\tTest F1:\t0.0551 *\n",
      "4 / 30: Train Loss:\t7.5680\tVal Loss:\t28.0023\tAccuracy:\t0.1686\tF1:\t0.0770\tTest Accuracy:\t0.1376\tTest F1:\t0.0620 *\n",
      "5 / 30: Train Loss:\t6.1189\tVal Loss:\t22.3440\tAccuracy:\t0.1686\tF1:\t0.1005\tTest Accuracy:\t0.1376\tTest F1:\t0.0791 *\n",
      "6 / 30: Train Loss:\t4.8132\tVal Loss:\t17.7214\tAccuracy:\t0.1982\tF1:\t0.1294\tTest Accuracy:\t0.1693\tTest F1:\t0.1086 *\n",
      "7 / 30: Train Loss:\t3.7516\tVal Loss:\t14.1844\tAccuracy:\t0.1923\tF1:\t0.1450\tTest Accuracy:\t0.1376\tTest F1:\t0.0882 *\n",
      "8 / 30: Train Loss:\t3.0242\tVal Loss:\t11.8706\tAccuracy:\t0.1923\tF1:\t0.1323\tTest Accuracy:\t0.1746\tTest F1:\t0.1209\n",
      "9 / 30: Train Loss:\t2.5923\tVal Loss:\t10.4110\tAccuracy:\t0.2308\tF1:\t0.1599\tTest Accuracy:\t0.2011\tTest F1:\t0.1325 *\n",
      "10 / 30: Train Loss:\t2.2202\tVal Loss:\t8.7968\tAccuracy:\t0.2456\tF1:\t0.1661\tTest Accuracy:\t0.2063\tTest F1:\t0.1343 *\n",
      "11 / 30: Train Loss:\t1.8213\tVal Loss:\t7.0043\tAccuracy:\t0.2751\tF1:\t0.1852\tTest Accuracy:\t0.2063\tTest F1:\t0.1350 *\n",
      "12 / 30: Train Loss:\t1.3615\tVal Loss:\t4.8930\tAccuracy:\t0.2899\tF1:\t0.1984\tTest Accuracy:\t0.2434\tTest F1:\t0.1632 *\n",
      "13 / 30: Train Loss:\t0.9022\tVal Loss:\t3.5148\tAccuracy:\t0.3609\tF1:\t0.2682\tTest Accuracy:\t0.3175\tTest F1:\t0.2253 *\n",
      "14 / 30: Train Loss:\t0.7736\tVal Loss:\t3.2881\tAccuracy:\t0.3698\tF1:\t0.2800\tTest Accuracy:\t0.3439\tTest F1:\t0.2576 *\n",
      "15 / 30: Train Loss:\t0.7208\tVal Loss:\t3.0945\tAccuracy:\t0.3846\tF1:\t0.2908\tTest Accuracy:\t0.3704\tTest F1:\t0.2773 *\n",
      "16 / 30: Train Loss:\t0.6783\tVal Loss:\t2.9456\tAccuracy:\t0.4024\tF1:\t0.3066\tTest Accuracy:\t0.3704\tTest F1:\t0.2802 *\n",
      "17 / 30: Train Loss:\t0.6412\tVal Loss:\t2.8111\tAccuracy:\t0.4290\tF1:\t0.3376\tTest Accuracy:\t0.3704\tTest F1:\t0.2808 *\n",
      "18 / 30: Train Loss:\t0.6104\tVal Loss:\t2.7035\tAccuracy:\t0.4172\tF1:\t0.3270\tTest Accuracy:\t0.4021\tTest F1:\t0.3156\n",
      "19 / 30: Train Loss:\t0.5815\tVal Loss:\t2.6110\tAccuracy:\t0.4438\tF1:\t0.3567\tTest Accuracy:\t0.3704\tTest F1:\t0.2898 *\n",
      "20 / 30: Train Loss:\t0.5567\tVal Loss:\t2.5400\tAccuracy:\t0.4467\tF1:\t0.3530\tTest Accuracy:\t0.3915\tTest F1:\t0.3105\n",
      "21 / 30: Train Loss:\t0.5351\tVal Loss:\t2.4572\tAccuracy:\t0.4379\tF1:\t0.3512\tTest Accuracy:\t0.3968\tTest F1:\t0.3182\n",
      "22 / 30: Train Loss:\t0.5157\tVal Loss:\t2.3814\tAccuracy:\t0.4586\tF1:\t0.3635\tTest Accuracy:\t0.4339\tTest F1:\t0.3492 *\n",
      "23 / 30: Train Loss:\t0.4964\tVal Loss:\t2.3138\tAccuracy:\t0.4645\tF1:\t0.3688\tTest Accuracy:\t0.4444\tTest F1:\t0.3578 *\n",
      "24 / 30: Train Loss:\t0.4801\tVal Loss:\t2.2551\tAccuracy:\t0.4586\tF1:\t0.3649\tTest Accuracy:\t0.4339\tTest F1:\t0.3479\n",
      "25 / 30: Train Loss:\t0.4651\tVal Loss:\t2.1981\tAccuracy:\t0.4822\tF1:\t0.3824\tTest Accuracy:\t0.4180\tTest F1:\t0.3316 *\n",
      "26 / 30: Train Loss:\t0.4506\tVal Loss:\t2.1500\tAccuracy:\t0.4734\tF1:\t0.3721\tTest Accuracy:\t0.4497\tTest F1:\t0.3617\n",
      "27 / 30: Train Loss:\t0.4359\tVal Loss:\t2.0920\tAccuracy:\t0.4763\tF1:\t0.3742\tTest Accuracy:\t0.4497\tTest F1:\t0.3532\n",
      "28 / 30: Train Loss:\t0.4234\tVal Loss:\t2.0572\tAccuracy:\t0.4852\tF1:\t0.3805\tTest Accuracy:\t0.4603\tTest F1:\t0.3700\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ab9a6d763a4defa7f9305227ffa86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/338 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t9.4688\tVal Loss:\t24.3176\tAccuracy:\t0.1450\tF1:\t0.0910\tTest Accuracy:\t0.1164\tTest F1:\t0.0734 *\n",
      "2 / 30: Train Loss:\t3.7984\tVal Loss:\t10.6198\tAccuracy:\t0.2249\tF1:\t0.1526\tTest Accuracy:\t0.1852\tTest F1:\t0.1209 *\n",
      "3 / 30: Train Loss:\t1.6033\tVal Loss:\t3.4599\tAccuracy:\t0.3580\tF1:\t0.2466\tTest Accuracy:\t0.3122\tTest F1:\t0.2039 *\n",
      "4 / 30: Train Loss:\t0.6871\tVal Loss:\t2.6837\tAccuracy:\t0.3994\tF1:\t0.3057\tTest Accuracy:\t0.3651\tTest F1:\t0.2804 *\n",
      "5 / 30: Train Loss:\t0.5389\tVal Loss:\t2.2910\tAccuracy:\t0.4586\tF1:\t0.3673\tTest Accuracy:\t0.4021\tTest F1:\t0.3117 *\n",
      "6 / 30: Train Loss:\t0.4599\tVal Loss:\t2.0499\tAccuracy:\t0.4675\tF1:\t0.3727\tTest Accuracy:\t0.4868\tTest F1:\t0.3899 *\n",
      "7 / 30: Train Loss:\t0.4034\tVal Loss:\t1.8765\tAccuracy:\t0.4793\tF1:\t0.3869\tTest Accuracy:\t0.5079\tTest F1:\t0.3922 *\n",
      "8 / 30: Train Loss:\t0.3582\tVal Loss:\t1.7456\tAccuracy:\t0.4911\tF1:\t0.3952\tTest Accuracy:\t0.5079\tTest F1:\t0.3979 *\n",
      "9 / 30: Train Loss:\t0.3220\tVal Loss:\t1.6348\tAccuracy:\t0.5296\tF1:\t0.4494\tTest Accuracy:\t0.5291\tTest F1:\t0.3954 *\n",
      "10 / 30: Train Loss:\t0.2909\tVal Loss:\t1.5517\tAccuracy:\t0.5325\tF1:\t0.4653\tTest Accuracy:\t0.5344\tTest F1:\t0.4404 *\n",
      "11 / 30: Train Loss:\t0.2633\tVal Loss:\t1.4853\tAccuracy:\t0.5473\tF1:\t0.4849\tTest Accuracy:\t0.5556\tTest F1:\t0.4680 *\n",
      "12 / 30: Train Loss:\t0.2394\tVal Loss:\t1.4389\tAccuracy:\t0.5769\tF1:\t0.5169\tTest Accuracy:\t0.5767\tTest F1:\t0.4869 *\n",
      "13 / 30: Train Loss:\t0.2185\tVal Loss:\t1.3921\tAccuracy:\t0.5917\tF1:\t0.5358\tTest Accuracy:\t0.5661\tTest F1:\t0.4714 *\n",
      "14 / 30: Train Loss:\t0.1988\tVal Loss:\t1.3639\tAccuracy:\t0.5947\tF1:\t0.5432\tTest Accuracy:\t0.5714\tTest F1:\t0.4758 *\n",
      "15 / 30: Train Loss:\t0.1816\tVal Loss:\t1.3302\tAccuracy:\t0.6183\tF1:\t0.5788\tTest Accuracy:\t0.5873\tTest F1:\t0.4985 *\n",
      "16 / 30: Train Loss:\t0.1640\tVal Loss:\t1.3292\tAccuracy:\t0.6124\tF1:\t0.5702\tTest Accuracy:\t0.5873\tTest F1:\t0.4929\n",
      "17 / 30: Train Loss:\t0.1479\tVal Loss:\t1.3219\tAccuracy:\t0.6243\tF1:\t0.5935\tTest Accuracy:\t0.5873\tTest F1:\t0.5104 *\n",
      "18 / 30: Train Loss:\t0.1313\tVal Loss:\t1.3256\tAccuracy:\t0.6272\tF1:\t0.5938\tTest Accuracy:\t0.5767\tTest F1:\t0.5002 *\n",
      "19 / 30: Train Loss:\t0.1160\tVal Loss:\t1.3484\tAccuracy:\t0.6124\tF1:\t0.5772\tTest Accuracy:\t0.5926\tTest F1:\t0.5220\n",
      "20 / 30: Train Loss:\t0.1017\tVal Loss:\t1.3668\tAccuracy:\t0.6095\tF1:\t0.5606\tTest Accuracy:\t0.5979\tTest F1:\t0.5299\n",
      "21 / 30: Train Loss:\t0.0869\tVal Loss:\t1.3624\tAccuracy:\t0.6036\tF1:\t0.5574\tTest Accuracy:\t0.6032\tTest F1:\t0.5278\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t6.9935\tVal Loss:\t12.2139\tAccuracy:\t0.2249\tF1:\t0.1103\tTest Accuracy:\t0.2011\tTest F1:\t0.0925 *\n",
      "2 / 30: Train Loss:\t1.5308\tVal Loss:\t3.2702\tAccuracy:\t0.3432\tF1:\t0.2556\tTest Accuracy:\t0.3122\tTest F1:\t0.2397 *\n",
      "3 / 30: Train Loss:\t0.5831\tVal Loss:\t2.2499\tAccuracy:\t0.4586\tF1:\t0.3679\tTest Accuracy:\t0.4286\tTest F1:\t0.3293 *\n",
      "4 / 30: Train Loss:\t0.4398\tVal Loss:\t1.9173\tAccuracy:\t0.5000\tF1:\t0.3993\tTest Accuracy:\t0.4921\tTest F1:\t0.3758 *\n",
      "5 / 30: Train Loss:\t0.3589\tVal Loss:\t1.6828\tAccuracy:\t0.5266\tF1:\t0.4438\tTest Accuracy:\t0.5238\tTest F1:\t0.4101 *\n",
      "6 / 30: Train Loss:\t0.2999\tVal Loss:\t1.5377\tAccuracy:\t0.5444\tF1:\t0.4701\tTest Accuracy:\t0.5556\tTest F1:\t0.4613 *\n",
      "7 / 30: Train Loss:\t0.2521\tVal Loss:\t1.4546\tAccuracy:\t0.5651\tF1:\t0.5065\tTest Accuracy:\t0.5556\tTest F1:\t0.4731 *\n",
      "8 / 30: Train Loss:\t0.2174\tVal Loss:\t1.3840\tAccuracy:\t0.5917\tF1:\t0.5293\tTest Accuracy:\t0.5820\tTest F1:\t0.4964 *\n",
      "9 / 30: Train Loss:\t0.1844\tVal Loss:\t1.3213\tAccuracy:\t0.6124\tF1:\t0.5788\tTest Accuracy:\t0.6138\tTest F1:\t0.5213 *\n",
      "10 / 30: Train Loss:\t0.1551\tVal Loss:\t1.3033\tAccuracy:\t0.6036\tF1:\t0.5612\tTest Accuracy:\t0.6349\tTest F1:\t0.5561\n",
      "11 / 30: Train Loss:\t0.1254\tVal Loss:\t1.3090\tAccuracy:\t0.6183\tF1:\t0.5916\tTest Accuracy:\t0.6349\tTest F1:\t0.5634 *\n",
      "12 / 30: Train Loss:\t0.0991\tVal Loss:\t1.2995\tAccuracy:\t0.6420\tF1:\t0.6124\tTest Accuracy:\t0.6349\tTest F1:\t0.5646 *\n",
      "13 / 30: Train Loss:\t0.0759\tVal Loss:\t1.3244\tAccuracy:\t0.6124\tF1:\t0.5898\tTest Accuracy:\t0.6190\tTest F1:\t0.5688\n",
      "14 / 30: Train Loss:\t0.0563\tVal Loss:\t1.4432\tAccuracy:\t0.6243\tF1:\t0.6049\tTest Accuracy:\t0.6085\tTest F1:\t0.5693\n",
      "15 / 30: Train Loss:\t0.0428\tVal Loss:\t1.5080\tAccuracy:\t0.6213\tF1:\t0.6017\tTest Accuracy:\t0.6402\tTest F1:\t0.5987\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t2.8360\tVal Loss:\t3.2653\tAccuracy:\t0.4704\tF1:\t0.3303\tTest Accuracy:\t0.4339\tTest F1:\t0.2821 *\n",
      "2 / 30: Train Loss:\t0.4599\tVal Loss:\t1.6729\tAccuracy:\t0.5503\tF1:\t0.5322\tTest Accuracy:\t0.6085\tTest F1:\t0.5467 *\n",
      "3 / 30: Train Loss:\t0.2517\tVal Loss:\t2.2774\tAccuracy:\t0.6213\tF1:\t0.5629\tTest Accuracy:\t0.5344\tTest F1:\t0.4736 *\n",
      "4 / 30: Train Loss:\t0.1882\tVal Loss:\t2.8577\tAccuracy:\t0.5592\tF1:\t0.5329\tTest Accuracy:\t0.5185\tTest F1:\t0.4944\n",
      "5 / 30: Train Loss:\t0.1147\tVal Loss:\t2.8365\tAccuracy:\t0.6213\tF1:\t0.5475\tTest Accuracy:\t0.6349\tTest F1:\t0.5716\n",
      "6 / 30: Train Loss:\t0.0980\tVal Loss:\t2.4774\tAccuracy:\t0.6479\tF1:\t0.6044\tTest Accuracy:\t0.7249\tTest F1:\t0.6674 *\n",
      "7 / 30: Train Loss:\t0.0487\tVal Loss:\t2.3929\tAccuracy:\t0.6716\tF1:\t0.6314\tTest Accuracy:\t0.6455\tTest F1:\t0.6105 *\n",
      "8 / 30: Train Loss:\t0.0436\tVal Loss:\t2.9065\tAccuracy:\t0.6154\tF1:\t0.6206\tTest Accuracy:\t0.6138\tTest F1:\t0.5707\n",
      "9 / 30: Train Loss:\t0.0231\tVal Loss:\t3.3948\tAccuracy:\t0.6538\tF1:\t0.6373\tTest Accuracy:\t0.6190\tTest F1:\t0.5777 *\n",
      "10 / 30: Train Loss:\t0.0113\tVal Loss:\t3.0073\tAccuracy:\t0.6686\tF1:\t0.6447\tTest Accuracy:\t0.6667\tTest F1:\t0.5980 *\n",
      "11 / 30: Train Loss:\t0.0178\tVal Loss:\t3.0525\tAccuracy:\t0.6657\tF1:\t0.5969\tTest Accuracy:\t0.6667\tTest F1:\t0.5968\n",
      "12 / 30: Train Loss:\t0.0027\tVal Loss:\t2.9819\tAccuracy:\t0.6627\tF1:\t0.6218\tTest Accuracy:\t0.6878\tTest F1:\t0.6244\n",
      "13 / 30: Train Loss:\t0.0009\tVal Loss:\t2.9174\tAccuracy:\t0.6864\tF1:\t0.6397\tTest Accuracy:\t0.6825\tTest F1:\t0.6230\n",
      "No improvement for 3 epochs. Stopping early.\n"
     ]
    }
   ],
   "source": [
    "r = 8\n",
    "a = 8\n",
    "lora_dropout = 0.0\n",
    "\n",
    "learning_rates = [1.0e-6, 1.0e-5, 5.0e-5 ,1.0e-4, 1.0e-3]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'Grid search {model_checkpoint}, learning rate {lr}')\n",
    "    data = ClimateDataset(model_to_train=3,model_checkpoint=model_checkpoint,dataset_url=flicc_path,batch_size=8)\n",
    "    data.setup_dataloaders()\n",
    "    model = ClassificationModel(model_checkpoint=data.model_checkpoint,\n",
    "                                num_labels=data.num_labels,\n",
    "                                quantized_model=True,\n",
    "                                lora=True,\n",
    "                                r=r,\n",
    "                                alpha=a,\n",
    "                                dropout=lora_dropout)\n",
    "    trainer = Engine(epochs=30,labels=data.labels)\n",
    "    trainer.model = model.model\n",
    "    trainer.run(lr=lr,\n",
    "                wd=0.0,\n",
    "                train_dataloader=data.train_dataloader,\n",
    "                eval_dataloader=data.eval_dataloader,\n",
    "                test_dataloader=data.test_dataloader,\n",
    "                accumulation_steps=4,\n",
    "                early_stop=3,\n",
    "                is_quantized=True)\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    del data, model, trainer\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
