{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/software/'\n",
    "import sys\n",
    "import gc\n",
    "# assuming data, models, engine in flicc directory:\n",
    "flicc_path = os.path.realpath(\"__file__\").split('grid_search')[0]\n",
    "sys.path.append(flicc_path)\n",
    "import torch\n",
    "from data import ClimateDataset\n",
    "from models import ClassificationModel\n",
    "from engine import Engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint='bigscience/bloom-560m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'test_acc':[],\n",
    "           'test_f1':[],\n",
    "           'eval_acc':[],\n",
    "           'eval_f1':[],\n",
    "           'lr':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search bigscience/bloom-560m, learning rate 1e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t22.9957\tVal Loss:\t92.3443\tAccuracy:\t0.2604\tF1:\t0.2066\tTest Accuracy:\t0.2656\tTest F1:\t0.2133 *\n",
      "2 / 30: Train Loss:\t22.7102\tVal Loss:\t91.1843\tAccuracy:\t0.2604\tF1:\t0.2066\tTest Accuracy:\t0.2656\tTest F1:\t0.2133\n",
      "3 / 30: Train Loss:\t22.4231\tVal Loss:\t90.0018\tAccuracy:\t0.2604\tF1:\t0.2066\tTest Accuracy:\t0.2656\tTest F1:\t0.2133\n",
      "4 / 30: Train Loss:\t22.1329\tVal Loss:\t88.7989\tAccuracy:\t0.2604\tF1:\t0.2066\tTest Accuracy:\t0.2656\tTest F1:\t0.2133\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t21.7221\tVal Loss:\t81.5378\tAccuracy:\t0.2604\tF1:\t0.2066\tTest Accuracy:\t0.2656\tTest F1:\t0.2133 *\n",
      "2 / 30: Train Loss:\t18.6143\tVal Loss:\t68.2674\tAccuracy:\t0.2604\tF1:\t0.2066\tTest Accuracy:\t0.2656\tTest F1:\t0.2133\n",
      "3 / 30: Train Loss:\t15.0582\tVal Loss:\t52.7688\tAccuracy:\t0.2626\tF1:\t0.2099\tTest Accuracy:\t0.2656\tTest F1:\t0.2133 *\n",
      "4 / 30: Train Loss:\t10.8518\tVal Loss:\t34.0436\tAccuracy:\t0.2626\tF1:\t0.2099\tTest Accuracy:\t0.2656\tTest F1:\t0.2133\n",
      "5 / 30: Train Loss:\t5.4844\tVal Loss:\t9.0777\tAccuracy:\t0.3042\tF1:\t0.2849\tTest Accuracy:\t0.3242\tTest F1:\t0.3064 *\n",
      "6 / 30: Train Loss:\t1.1292\tVal Loss:\t4.0131\tAccuracy:\t0.5755\tF1:\t0.4548\tTest Accuracy:\t0.5820\tTest F1:\t0.4512 *\n",
      "7 / 30: Train Loss:\t0.8124\tVal Loss:\t3.4888\tAccuracy:\t0.5755\tF1:\t0.4686\tTest Accuracy:\t0.5781\tTest F1:\t0.4643 *\n",
      "8 / 30: Train Loss:\t0.7060\tVal Loss:\t3.0379\tAccuracy:\t0.5930\tF1:\t0.4854\tTest Accuracy:\t0.6016\tTest F1:\t0.4941 *\n",
      "9 / 30: Train Loss:\t0.6219\tVal Loss:\t2.7318\tAccuracy:\t0.6039\tF1:\t0.4979\tTest Accuracy:\t0.5898\tTest F1:\t0.4949 *\n",
      "10 / 30: Train Loss:\t0.5540\tVal Loss:\t2.4633\tAccuracy:\t0.6214\tF1:\t0.5201\tTest Accuracy:\t0.6055\tTest F1:\t0.5181 *\n",
      "11 / 30: Train Loss:\t0.5014\tVal Loss:\t2.2385\tAccuracy:\t0.6389\tF1:\t0.5491\tTest Accuracy:\t0.6133\tTest F1:\t0.5351 *\n",
      "12 / 30: Train Loss:\t0.4556\tVal Loss:\t2.0499\tAccuracy:\t0.6433\tF1:\t0.5479\tTest Accuracy:\t0.6328\tTest F1:\t0.5568\n",
      "13 / 30: Train Loss:\t0.4196\tVal Loss:\t1.9101\tAccuracy:\t0.6455\tF1:\t0.5562\tTest Accuracy:\t0.6328\tTest F1:\t0.5635 *\n",
      "14 / 30: Train Loss:\t0.3874\tVal Loss:\t1.7759\tAccuracy:\t0.6499\tF1:\t0.5638\tTest Accuracy:\t0.6406\tTest F1:\t0.5728 *\n",
      "15 / 30: Train Loss:\t0.3641\tVal Loss:\t1.6872\tAccuracy:\t0.6455\tF1:\t0.5584\tTest Accuracy:\t0.6367\tTest F1:\t0.5697\n",
      "16 / 30: Train Loss:\t0.3425\tVal Loss:\t1.5728\tAccuracy:\t0.6499\tF1:\t0.5638\tTest Accuracy:\t0.6523\tTest F1:\t0.5883\n",
      "17 / 30: Train Loss:\t0.3206\tVal Loss:\t1.5013\tAccuracy:\t0.6455\tF1:\t0.5584\tTest Accuracy:\t0.6523\tTest F1:\t0.5912\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t15.2983\tVal Loss:\t21.4864\tAccuracy:\t0.2626\tF1:\t0.2099\tTest Accuracy:\t0.2656\tTest F1:\t0.2133 *\n",
      "2 / 30: Train Loss:\t1.2678\tVal Loss:\t2.8135\tAccuracy:\t0.5536\tF1:\t0.5052\tTest Accuracy:\t0.5625\tTest F1:\t0.5293 *\n",
      "3 / 30: Train Loss:\t0.4640\tVal Loss:\t1.7695\tAccuracy:\t0.6455\tF1:\t0.5584\tTest Accuracy:\t0.6445\tTest F1:\t0.5759 *\n",
      "4 / 30: Train Loss:\t0.3336\tVal Loss:\t1.3785\tAccuracy:\t0.6608\tF1:\t0.5764\tTest Accuracy:\t0.6758\tTest F1:\t0.6072 *\n",
      "5 / 30: Train Loss:\t0.2668\tVal Loss:\t1.1593\tAccuracy:\t0.6761\tF1:\t0.5946\tTest Accuracy:\t0.6875\tTest F1:\t0.6198 *\n",
      "6 / 30: Train Loss:\t0.2229\tVal Loss:\t1.0022\tAccuracy:\t0.6827\tF1:\t0.6018\tTest Accuracy:\t0.7148\tTest F1:\t0.6545 *\n",
      "7 / 30: Train Loss:\t0.1911\tVal Loss:\t0.8906\tAccuracy:\t0.7002\tF1:\t0.6219\tTest Accuracy:\t0.7148\tTest F1:\t0.6545 *\n",
      "8 / 30: Train Loss:\t0.1667\tVal Loss:\t0.8224\tAccuracy:\t0.7177\tF1:\t0.6365\tTest Accuracy:\t0.7266\tTest F1:\t0.6700 *\n",
      "9 / 30: Train Loss:\t0.1472\tVal Loss:\t0.7840\tAccuracy:\t0.7374\tF1:\t0.6645\tTest Accuracy:\t0.7227\tTest F1:\t0.6691 *\n",
      "10 / 30: Train Loss:\t0.1336\tVal Loss:\t0.7259\tAccuracy:\t0.7440\tF1:\t0.6738\tTest Accuracy:\t0.7305\tTest F1:\t0.6734 *\n",
      "11 / 30: Train Loss:\t0.1208\tVal Loss:\t0.6981\tAccuracy:\t0.7484\tF1:\t0.6777\tTest Accuracy:\t0.7461\tTest F1:\t0.6847 *\n",
      "12 / 30: Train Loss:\t0.1116\tVal Loss:\t0.6786\tAccuracy:\t0.7615\tF1:\t0.6976\tTest Accuracy:\t0.7422\tTest F1:\t0.6812 *\n",
      "13 / 30: Train Loss:\t0.1021\tVal Loss:\t0.6519\tAccuracy:\t0.7681\tF1:\t0.7037\tTest Accuracy:\t0.7578\tTest F1:\t0.7005 *\n",
      "14 / 30: Train Loss:\t0.0961\tVal Loss:\t0.6427\tAccuracy:\t0.7615\tF1:\t0.6895\tTest Accuracy:\t0.7461\tTest F1:\t0.6791\n",
      "15 / 30: Train Loss:\t0.0905\tVal Loss:\t0.6334\tAccuracy:\t0.7681\tF1:\t0.7052\tTest Accuracy:\t0.7500\tTest F1:\t0.6854 *\n",
      "16 / 30: Train Loss:\t0.0852\tVal Loss:\t0.6180\tAccuracy:\t0.7724\tF1:\t0.7030\tTest Accuracy:\t0.7500\tTest F1:\t0.6796\n",
      "17 / 30: Train Loss:\t0.0788\tVal Loss:\t0.6084\tAccuracy:\t0.7746\tF1:\t0.7082\tTest Accuracy:\t0.7617\tTest F1:\t0.6960 *\n",
      "18 / 30: Train Loss:\t0.0735\tVal Loss:\t0.6055\tAccuracy:\t0.7812\tF1:\t0.7128\tTest Accuracy:\t0.7617\tTest F1:\t0.6902 *\n",
      "19 / 30: Train Loss:\t0.0684\tVal Loss:\t0.5972\tAccuracy:\t0.7790\tF1:\t0.7091\tTest Accuracy:\t0.7695\tTest F1:\t0.7032\n",
      "20 / 30: Train Loss:\t0.0634\tVal Loss:\t0.5923\tAccuracy:\t0.7768\tF1:\t0.7071\tTest Accuracy:\t0.7656\tTest F1:\t0.6996\n",
      "21 / 30: Train Loss:\t0.0592\tVal Loss:\t0.5909\tAccuracy:\t0.7899\tF1:\t0.7227\tTest Accuracy:\t0.7734\tTest F1:\t0.7096 *\n",
      "22 / 30: Train Loss:\t0.0537\tVal Loss:\t0.5856\tAccuracy:\t0.7877\tF1:\t0.7252\tTest Accuracy:\t0.7773\tTest F1:\t0.7186 *\n",
      "23 / 30: Train Loss:\t0.0491\tVal Loss:\t0.5892\tAccuracy:\t0.7987\tF1:\t0.7358\tTest Accuracy:\t0.7695\tTest F1:\t0.7087 *\n",
      "24 / 30: Train Loss:\t0.0445\tVal Loss:\t0.5970\tAccuracy:\t0.8009\tF1:\t0.7379\tTest Accuracy:\t0.7812\tTest F1:\t0.7222 *\n",
      "25 / 30: Train Loss:\t0.0399\tVal Loss:\t0.6057\tAccuracy:\t0.7899\tF1:\t0.7344\tTest Accuracy:\t0.7695\tTest F1:\t0.7113\n",
      "26 / 30: Train Loss:\t0.0356\tVal Loss:\t0.6294\tAccuracy:\t0.7768\tF1:\t0.7284\tTest Accuracy:\t0.7500\tTest F1:\t0.6959\n",
      "27 / 30: Train Loss:\t0.0311\tVal Loss:\t0.6606\tAccuracy:\t0.7702\tF1:\t0.7234\tTest Accuracy:\t0.7227\tTest F1:\t0.6715\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t8.9056\tVal Loss:\t3.2103\tAccuracy:\t0.6696\tF1:\t0.4308\tTest Accuracy:\t0.6992\tTest F1:\t0.4569 *\n",
      "2 / 30: Train Loss:\t0.4474\tVal Loss:\t1.4815\tAccuracy:\t0.6608\tF1:\t0.5764\tTest Accuracy:\t0.6758\tTest F1:\t0.6102 *\n",
      "3 / 30: Train Loss:\t0.2775\tVal Loss:\t1.1067\tAccuracy:\t0.6718\tF1:\t0.6058\tTest Accuracy:\t0.6836\tTest F1:\t0.6378 *\n",
      "4 / 30: Train Loss:\t0.2061\tVal Loss:\t0.9344\tAccuracy:\t0.7046\tF1:\t0.6474\tTest Accuracy:\t0.6992\tTest F1:\t0.6599 *\n",
      "5 / 30: Train Loss:\t0.1647\tVal Loss:\t0.7773\tAccuracy:\t0.7505\tF1:\t0.6744\tTest Accuracy:\t0.7227\tTest F1:\t0.6612 *\n",
      "6 / 30: Train Loss:\t0.1350\tVal Loss:\t0.7041\tAccuracy:\t0.7505\tF1:\t0.6544\tTest Accuracy:\t0.7773\tTest F1:\t0.6984\n",
      "7 / 30: Train Loss:\t0.1189\tVal Loss:\t0.6730\tAccuracy:\t0.7659\tF1:\t0.6612\tTest Accuracy:\t0.7656\tTest F1:\t0.6656\n",
      "8 / 30: Train Loss:\t0.1075\tVal Loss:\t0.6589\tAccuracy:\t0.7746\tF1:\t0.6643\tTest Accuracy:\t0.7656\tTest F1:\t0.6571\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search bigscience/bloom-560m, learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t2.1609\tVal Loss:\t1.1750\tAccuracy:\t0.4267\tF1:\t0.4262\tTest Accuracy:\t0.3516\tTest F1:\t0.3467 *\n",
      "2 / 30: Train Loss:\t0.1692\tVal Loss:\t1.0329\tAccuracy:\t0.4836\tF1:\t0.4836\tTest Accuracy:\t0.4453\tTest F1:\t0.4450 *\n",
      "3 / 30: Train Loss:\t0.1707\tVal Loss:\t0.7733\tAccuracy:\t0.6433\tF1:\t0.6289\tTest Accuracy:\t0.6094\tTest F1:\t0.5951 *\n",
      "4 / 30: Train Loss:\t0.1015\tVal Loss:\t0.4593\tAccuracy:\t0.8031\tF1:\t0.7055\tTest Accuracy:\t0.7539\tTest F1:\t0.6423 *\n",
      "5 / 30: Train Loss:\t0.0601\tVal Loss:\t1.1201\tAccuracy:\t0.5777\tF1:\t0.5744\tTest Accuracy:\t0.5234\tTest F1:\t0.5216\n",
      "6 / 30: Train Loss:\t0.1137\tVal Loss:\t0.9752\tAccuracy:\t0.7155\tF1:\t0.6909\tTest Accuracy:\t0.6797\tTest F1:\t0.6522\n",
      "7 / 30: Train Loss:\t0.1707\tVal Loss:\t3.1637\tAccuracy:\t0.7593\tF1:\t0.5003\tTest Accuracy:\t0.7461\tTest F1:\t0.4686\n",
      "No improvement for 3 epochs. Stopping early.\n"
     ]
    }
   ],
   "source": [
    "r = 8\n",
    "a = 8\n",
    "lora_dropout = 0.0\n",
    "\n",
    "learning_rates = [1.0e-6, 1.0e-5, 5.0e-5 ,1.0e-4, 1.0e-3]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'Grid search {model_checkpoint}, learning rate {lr}')\n",
    "    data = ClimateDataset(model_to_train=1,model_checkpoint=model_checkpoint,dataset_url=flicc_path,batch_size=8)\n",
    "    data.setup_dataloaders()\n",
    "    model = ClassificationModel(model_checkpoint=data.model_checkpoint,\n",
    "                                num_labels=data.num_labels,\n",
    "                                quantized_model=True,\n",
    "                                lora=True,\n",
    "                                r=r,\n",
    "                                alpha=a,\n",
    "                                dropout=lora_dropout)\n",
    "    trainer = Engine(epochs=30,labels=data.labels)\n",
    "    trainer.model = model.model\n",
    "    trainer.run(lr=lr,\n",
    "                wd=0.0,\n",
    "                train_dataloader=data.train_dataloader,\n",
    "                eval_dataloader=data.eval_dataloader,\n",
    "                test_dataloader=data.test_dataloader,\n",
    "                accumulation_steps=4,\n",
    "                early_stop=3,\n",
    "                is_quantized=True)\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    del data, model, trainer\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
