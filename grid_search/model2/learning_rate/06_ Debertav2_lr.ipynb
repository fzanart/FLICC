{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/mnt/software/'\n",
    "import sys\n",
    "import gc\n",
    "# assuming data, models, engine in flicc directory:\n",
    "flicc_path = os.path.realpath(\"__file__\").split('grid_search')[0]\n",
    "sys.path.append(flicc_path)\n",
    "import torch\n",
    "from data import ClimateDataset\n",
    "from models import ClassificationModel\n",
    "from engine import Engine\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint='microsoft/deberta-v2-xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'test_acc':[],\n",
    "           'test_f1':[],\n",
    "           'eval_acc':[],\n",
    "           'eval_f1':[],\n",
    "           'lr':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search microsoft/deberta-v2-xlarge, learning rate 1e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59831d85dee44f6388b16220c57b768d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0549fc6cf12c4d6993f367833625bec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/256 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf224d502ce465c8795dc403a965e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c8a4bada06476983e00dbbf9354ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa30aeb8be344a6b61bb21e755a2567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468d0ee161544942993377b2257b8231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.2782\tVal Loss:\t1.1085\tAccuracy:\t0.3445\tF1:\t0.2652\tTest Accuracy:\t0.3731\tTest F1:\t0.2807 *\n",
      "2 / 30: Train Loss:\t0.2754\tVal Loss:\t1.1095\tAccuracy:\t0.3277\tF1:\t0.2512\tTest Accuracy:\t0.3881\tTest F1:\t0.2885\n",
      "3 / 30: Train Loss:\t0.2791\tVal Loss:\t1.1093\tAccuracy:\t0.3361\tF1:\t0.2594\tTest Accuracy:\t0.4030\tTest F1:\t0.3107\n",
      "4 / 30: Train Loss:\t0.2764\tVal Loss:\t1.1077\tAccuracy:\t0.3697\tF1:\t0.2879\tTest Accuracy:\t0.4030\tTest F1:\t0.3107 *\n",
      "5 / 30: Train Loss:\t0.2803\tVal Loss:\t1.1089\tAccuracy:\t0.3529\tF1:\t0.2749\tTest Accuracy:\t0.4030\tTest F1:\t0.3036\n",
      "6 / 30: Train Loss:\t0.2800\tVal Loss:\t1.1089\tAccuracy:\t0.3613\tF1:\t0.2827\tTest Accuracy:\t0.3731\tTest F1:\t0.2782\n",
      "7 / 30: Train Loss:\t0.2765\tVal Loss:\t1.1081\tAccuracy:\t0.3529\tF1:\t0.2771\tTest Accuracy:\t0.3881\tTest F1:\t0.2937\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 1e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dccf7d0b164840a4cc7c0053b95c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.2784\tVal Loss:\t1.1067\tAccuracy:\t0.3697\tF1:\t0.2882\tTest Accuracy:\t0.3881\tTest F1:\t0.2945 *\n",
      "2 / 30: Train Loss:\t0.2752\tVal Loss:\t1.1052\tAccuracy:\t0.3782\tF1:\t0.2980\tTest Accuracy:\t0.3881\tTest F1:\t0.2976 *\n",
      "3 / 30: Train Loss:\t0.2780\tVal Loss:\t1.1074\tAccuracy:\t0.3445\tF1:\t0.2705\tTest Accuracy:\t0.4179\tTest F1:\t0.3267\n",
      "4 / 30: Train Loss:\t0.2742\tVal Loss:\t1.1052\tAccuracy:\t0.3445\tF1:\t0.2687\tTest Accuracy:\t0.5075\tTest F1:\t0.3977\n",
      "5 / 30: Train Loss:\t0.2786\tVal Loss:\t1.1045\tAccuracy:\t0.3613\tF1:\t0.2772\tTest Accuracy:\t0.4925\tTest F1:\t0.3860\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 5e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd6237eb2fc424884550c349402d436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.2773\tVal Loss:\t1.1024\tAccuracy:\t0.3866\tF1:\t0.2964\tTest Accuracy:\t0.4925\tTest F1:\t0.3855 *\n",
      "2 / 30: Train Loss:\t0.2738\tVal Loss:\t1.1011\tAccuracy:\t0.3697\tF1:\t0.1800\tTest Accuracy:\t0.3731\tTest F1:\t0.2256\n",
      "3 / 30: Train Loss:\t0.2754\tVal Loss:\t1.0993\tAccuracy:\t0.3782\tF1:\t0.1829\tTest Accuracy:\t0.3731\tTest F1:\t0.2064\n",
      "4 / 30: Train Loss:\t0.2720\tVal Loss:\t1.0969\tAccuracy:\t0.3782\tF1:\t0.1829\tTest Accuracy:\t0.3731\tTest F1:\t0.2056\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.2775\tVal Loss:\t1.1003\tAccuracy:\t0.3782\tF1:\t0.1829\tTest Accuracy:\t0.3881\tTest F1:\t0.2130 *\n",
      "2 / 30: Train Loss:\t0.2731\tVal Loss:\t1.0978\tAccuracy:\t0.3782\tF1:\t0.1829\tTest Accuracy:\t0.3881\tTest F1:\t0.2121\n",
      "3 / 30: Train Loss:\t0.2749\tVal Loss:\t1.0974\tAccuracy:\t0.3613\tF1:\t0.1924\tTest Accuracy:\t0.3731\tTest F1:\t0.2056 *\n",
      "4 / 30: Train Loss:\t0.2712\tVal Loss:\t1.0935\tAccuracy:\t0.3782\tF1:\t0.2757\tTest Accuracy:\t0.4627\tTest F1:\t0.3654 *\n",
      "5 / 30: Train Loss:\t0.2738\tVal Loss:\t1.0878\tAccuracy:\t0.3697\tF1:\t0.2603\tTest Accuracy:\t0.4179\tTest F1:\t0.3210\n",
      "6 / 30: Train Loss:\t0.2721\tVal Loss:\t1.0824\tAccuracy:\t0.3782\tF1:\t0.1977\tTest Accuracy:\t0.4478\tTest F1:\t0.3262\n",
      "7 / 30: Train Loss:\t0.2665\tVal Loss:\t1.0754\tAccuracy:\t0.3866\tF1:\t0.2011\tTest Accuracy:\t0.4328\tTest F1:\t0.3117\n",
      "No improvement for 3 epochs. Stopping early.\n",
      "Grid search microsoft/deberta-v2-xlarge, learning rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xlarge and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30: Train Loss:\t0.2786\tVal Loss:\t1.1025\tAccuracy:\t0.3697\tF1:\t0.3187\tTest Accuracy:\t0.4179\tTest F1:\t0.3677 *\n",
      "2 / 30: Train Loss:\t0.2631\tVal Loss:\t1.0392\tAccuracy:\t0.4202\tF1:\t0.3845\tTest Accuracy:\t0.4627\tTest F1:\t0.4556 *\n",
      "3 / 30: Train Loss:\t0.2383\tVal Loss:\t0.8042\tAccuracy:\t0.5966\tF1:\t0.5995\tTest Accuracy:\t0.5970\tTest F1:\t0.5969 *\n",
      "4 / 30: Train Loss:\t0.1853\tVal Loss:\t0.6643\tAccuracy:\t0.7395\tF1:\t0.7220\tTest Accuracy:\t0.6418\tTest F1:\t0.6479 *\n",
      "5 / 30: Train Loss:\t0.1258\tVal Loss:\t0.6134\tAccuracy:\t0.7563\tF1:\t0.7502\tTest Accuracy:\t0.7612\tTest F1:\t0.7650 *\n",
      "6 / 30: Train Loss:\t0.0859\tVal Loss:\t0.5710\tAccuracy:\t0.8067\tF1:\t0.7990\tTest Accuracy:\t0.7164\tTest F1:\t0.7158 *\n",
      "7 / 30: Train Loss:\t0.0635\tVal Loss:\t0.5426\tAccuracy:\t0.8067\tF1:\t0.8088\tTest Accuracy:\t0.7313\tTest F1:\t0.7325 *\n",
      "8 / 30: Train Loss:\t0.0441\tVal Loss:\t0.5769\tAccuracy:\t0.8319\tF1:\t0.8333\tTest Accuracy:\t0.7910\tTest F1:\t0.7890 *\n",
      "9 / 30: Train Loss:\t0.0230\tVal Loss:\t0.6279\tAccuracy:\t0.8403\tF1:\t0.8411\tTest Accuracy:\t0.8209\tTest F1:\t0.8221 *\n",
      "10 / 30: Train Loss:\t0.0078\tVal Loss:\t0.7000\tAccuracy:\t0.8655\tF1:\t0.8643\tTest Accuracy:\t0.8209\tTest F1:\t0.8221 *\n",
      "11 / 30: Train Loss:\t0.0068\tVal Loss:\t0.8644\tAccuracy:\t0.8067\tF1:\t0.8051\tTest Accuracy:\t0.7910\tTest F1:\t0.7949\n",
      "12 / 30: Train Loss:\t0.0114\tVal Loss:\t0.8839\tAccuracy:\t0.8067\tF1:\t0.8063\tTest Accuracy:\t0.8060\tTest F1:\t0.8085\n",
      "13 / 30: Train Loss:\t0.0163\tVal Loss:\t0.8884\tAccuracy:\t0.8067\tF1:\t0.8059\tTest Accuracy:\t0.8209\tTest F1:\t0.8221\n",
      "No improvement for 3 epochs. Stopping early.\n"
     ]
    }
   ],
   "source": [
    "r = 8\n",
    "a = 8\n",
    "lora_dropout = 0.0\n",
    "\n",
    "learning_rates = [1.0e-6, 1.0e-5, 5.0e-5 ,1.0e-4, 1.0e-3]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f'Grid search {model_checkpoint}, learning rate {lr}')\n",
    "    data = ClimateDataset(model_to_train=2,model_checkpoint=model_checkpoint,dataset_url=flicc_path,batch_size=8)\n",
    "    data.setup_dataloaders()\n",
    "    model = ClassificationModel(model_checkpoint=data.model_checkpoint,\n",
    "                                num_labels=data.num_labels,\n",
    "                                quantized_model=True,\n",
    "                                lora=True,\n",
    "                                r=r,\n",
    "                                alpha=a,\n",
    "                                dropout=lora_dropout)\n",
    "    trainer = Engine(epochs=30,labels=data.labels)\n",
    "    trainer.model = model.model\n",
    "    trainer.run(lr=lr,\n",
    "                wd=0.0,\n",
    "                train_dataloader=data.train_dataloader,\n",
    "                eval_dataloader=data.eval_dataloader,\n",
    "                test_dataloader=data.test_dataloader,\n",
    "                accumulation_steps=4,\n",
    "                early_stop=3,\n",
    "                is_quantized=True)\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    del data, model, trainer\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
